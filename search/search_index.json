{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"LLM Study Notes","text":"<p>Welcome to the study notes on Large Language Models (LLMs).</p>"},{"location":"#contents","title":"Contents","text":"<ul> <li>Introduction</li> <li>Transformers</li> <li>Attention Mechanism</li> <li>Training LLMs</li> </ul>"},{"location":"#contact","title":"Contact","text":"<p>If you have any questions, feel free to contact me at: Email me at </p> <p>22f3002638@ds.study.iitm.ac.in</p>"},{"location":"attention-mechanism/","title":"Attention Mechanism","text":"<p>Attention allows the model to focus on relevant parts of the input.</p>"},{"location":"attention-mechanism/#types","title":"Types","text":"<ul> <li>Self-Attention</li> <li>Multi-Head Attention</li> <li>Scaled Dot-Product Attention</li> </ul>"},{"location":"intro-to-llms/","title":"Introduction to Large Language Models","text":"<p>Large Language Models (LLMs) are deep learning models trained to understand and generate human language.</p>"},{"location":"intro-to-llms/#key-concepts","title":"Key Concepts","text":"<ul> <li>Pre-training and fine-tuning</li> <li>Tokenization</li> <li>Language modeling objectives</li> </ul>"},{"location":"training-llms/","title":"Training LLMs","text":"<p>Training involves large datasets and massive compute resources.</p>"},{"location":"training-llms/#techniques","title":"Techniques","text":"<ul> <li>Causal Language Modeling (CLM)</li> <li>Masked Language Modeling (MLM)</li> <li>Reinforcement Learning from Human Feedback (RLHF)</li> </ul>"},{"location":"transformers/","title":"Transformers","text":"<p>Transformers are the backbone of modern NLP models, introduced in the paper 'Attention is All You Need'.</p>"},{"location":"transformers/#components","title":"Components","text":"<ul> <li>Encoder and Decoder</li> <li>Self-Attention</li> <li>Feedforward Networks</li> <li>Layer Normalization</li> </ul>"}]}