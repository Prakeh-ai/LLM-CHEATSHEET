# Transformers

Transformers are the backbone of modern NLP models, introduced in the paper 'Attention is All You Need'.

## Components
- Encoder and Decoder
- Self-Attention
- Feedforward Networks
- Layer Normalization